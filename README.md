# Consumindo API e persistindo num Data Lakehouse
![#](https://img.shields.io/badge/license-Apache-blue)

CriaÃ§Ã£o de um Data Lakehouse utilizando arquitetura [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

Basicamente consumindo uma API, realizando tratamento dos dados e entregando tabelas agregadas.


## Arquitetura do fluxo
![Arquitetura Lakehouse](img/img_architetura.png)

## Data Source
Para consumir uma API foi utilizado o [Projeto Open Brewery DB](https://www.openbrewerydb.org/) que sÃ£o dados abertos de cervejarias.

## Data Lakehouse
Dividido nas seguintes camadas:

- ***Bronze*** - dados sem nenhuma transformaÃ§Ã£o e inserido metadado
- ***Silver*** - dados com alguma transformaÃ§Ã£o e inserido metadado
- ***Gold*** - dados agregados conforme regra de nÃ©gocio

Para essa soluÃ§Ã£o foi utilizado as seguintes ferramentas:
- ***Apache Spark*** - engine de processamento 
- ***Apache Airflow*** - orquestraÃ§Ã£o das DAGs
- ***Delta Table*** - camada de armazenamento otimizada

## Estrutura de pasta
* dags
* scripts
    * spark
    * sql

Na pasta `dags` temos o cÃ³digo reponsÃ¡vel pela definiÃ§Ã£o da DAG 

Na pasta `scripts/spark` temos o cÃ³digo pyspark responsÃ¡vel pela carga dos dados

Na pasta `scripts/sql` temos o *SELECT* para criaÃ§Ã£o da visÃ£o de negÃ³cio


***

## ExecuÃ§Ã£o das DAGs
A carga das tabelas Ã© realizado pela execuÃ§Ã£o da DAG no Airflow, conforme os passos a seguir.

1. Acessando a interface do Airflow <p>
Clique no link `Airflow UI` para ser direcionado
![Alt text](img/airflow_01.png)

2. Selecionado a DAG<p>
Clicar no link = `dag_pipeline_lakehouse` para acessar a DAG
![Alt text](img/airflow_10.png)
3. Executando a DAG<p>
**Agora sim**, clique no play para iniciar a execuÃ§Ã£o
![Alt text](img/airflow_14.png)
4. Acompanhamento execuÃ§Ã£o<p>
Uma nova coluna ğŸ”² de execuÃ§Ã£o serÃ¡ exibida
![Alt text](img/airflow_15.png)<p>
Progresso da execuÃ§Ã£o serÃ¡ sinalizada com a mudanÃ§a na cor da coluna (:green_square: indica execuÃ§Ã£o com sucesso)<p>
![Alt text](img/airflow_16.png)

> **Aguardar a finalizaÃ§Ã£o de todos os steps.**


***
## :rocket: Acessando Storage

:closed_lock_with_key: Conectando na Virtual Machine<p>
Para verificar os arquivos gerados assim como a estrutura de pastas, serÃ¡ necessÃ¡rio conectar na mÃ¡quina remota.

Abrir um terminal:
>_MÃ¡quina Windows instalar algum aplicativo para conexÃ£o ssh_.<p>
> SugestÃ£o utilize o putty [download here](https://www.putty.org/)

#### Step by step 
1. Realizar conexÃ£o remota via SSH
```bash
-> ssh lahouse@xx.xxx.xxx.xx
# ğŸ‘‡ ApÃ³s digitar a senha seu prompt deve ser alterado conforme tela. ğŸ‘‡
```
![Alt text](img/airflow_13.png)

2. Ir para o diretÃ³rio principal do lakehouse
```bash
lakehouse@ubuntu-s-4vcpu-8gb-intel-fra1-01:~$ cd lakehouse

# ğŸ“ Ãrvore de pastas conforme as camadas da arquitetura
.
â”œâ”€â”€ bronze
â”‚Â Â  â””â”€â”€ brewery
â”œâ”€â”€ gold
â”‚Â Â  â”œâ”€â”€ brewery_type_by_state
â”‚Â Â  â””â”€â”€ location_by_brewery_type
â””â”€â”€ silver
    â””â”€â”€ brewery
```
ğŸ“Œ Navegue entres as pastas para verificar os arquivos criados
***

<a href="https://www.digitalocean.com/?refcode=20e91ebaafe6&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge"><img src="https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg" alt="DigitalOcean Referral Badge" /></a>
